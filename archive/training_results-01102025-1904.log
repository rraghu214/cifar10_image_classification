█████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9248 Batch_id=668 Accuracy=71.14:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9248 Batch_id=668 Accuracy=71.14:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7849 Batch_id=669 Accuracy=71.14:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7412 Batch_id=670 Accuracy=71.15:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7412 Batch_id=670 Accuracy=71.15:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8612 Batch_id=671 Accuracy=71.15:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6419 Batch_id=672 Accuracy=71.16:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6419 Batch_id=672 Accuracy=71.16:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6375 Batch_id=673 Accuracy=71.17:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8790 Batch_id=674 Accuracy=71.18:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8790 Batch_id=674 Accuracy=71.18:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9078 Batch_id=675 Accuracy=71.17:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6747 Batch_id=676 Accuracy=71.17:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6747 Batch_id=676 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9662 Batch_id=677 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6499 Batch_id=678 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6499 Batch_id=678 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0937 Batch_id=679 Accuracy=71.16:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5043 Batch_id=680 Accuracy=71.18:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5043 Batch_id=680 Accuracy=71.18:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7386 Batch_id=681 Accuracy=71.18:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9513 Batch_id=682 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9513 Batch_id=682 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7623 Batch_id=683 Accuracy=71.17:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7358 Batch_id=684 Accuracy=71.18:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7358 Batch_id=684 Accuracy=71.18:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8115 Batch_id=685 Accuracy=71.18:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7460 Batch_id=686 Accuracy=71.18:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7460 Batch_id=686 Accuracy=71.18:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0080 Batch_id=687 Accuracy=71.17:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5453 Batch_id=688 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5453 Batch_id=688 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9450 Batch_id=689 Accuracy=71.18:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7326 Batch_id=690 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7326 Batch_id=690 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7767 Batch_id=691 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7839 Batch_id=692 Accuracy=71.19:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7839 Batch_id=692 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8297 Batch_id=693 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.1517 Batch_id=694 Accuracy=71.18:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.1517 Batch_id=694 Accuracy=71.18:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6789 Batch_id=695 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7342 Batch_id=696 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7342 Batch_id=696 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9137 Batch_id=697 Accuracy=71.18:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6958 Batch_id=698 Accuracy=71.18:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6958 Batch_id=698 Accuracy=71.18:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7537 Batch_id=699 Accuracy=71.19:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6690 Batch_id=700 Accuracy=71.20:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6690 Batch_id=700 Accuracy=71.20:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7516 Batch_id=701 Accuracy=71.21:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0397 Batch_id=702 Accuracy=71.20:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0397 Batch_id=702 Accuracy=71.20:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8850 Batch_id=703 Accuracy=71.18:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6457 Batch_id=704 Accuracy=71.19:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6457 Batch_id=704 Accuracy=71.19:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9570 Batch_id=705 Accuracy=71.18:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7823 Batch_id=706 Accuracy=71.18:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7823 Batch_id=706 Accuracy=71.18:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6792 Batch_id=707 Accuracy=71.18:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8748 Batch_id=708 Accuracy=71.19:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8748 Batch_id=708 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5751 Batch_id=709 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8091 Batch_id=710 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8091 Batch_id=710 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6576 Batch_id=711 Accuracy=71.20:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8584 Batch_id=712 Accuracy=71.20:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8584 Batch_id=712 Accuracy=71.20:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0215 Batch_id=713 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6945 Batch_id=714 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6945 Batch_id=714 Accuracy=71.19:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9979 Batch_id=715 Accuracy=71.18:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5642 Batch_id=716 Accuracy=71.20:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5642 Batch_id=716 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7818 Batch_id=717 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7174 Batch_id=718 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7174 Batch_id=718 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.1208 Batch_id=719 Accuracy=71.19:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7193 Batch_id=720 Accuracy=71.19:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7193 Batch_id=720 Accuracy=71.19:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0602 Batch_id=721 Accuracy=71.19:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5570 Batch_id=722 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5570 Batch_id=722 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8782 Batch_id=723 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6665 Batch_id=724 Accuracy=71.20:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6665 Batch_id=724 Accuracy=71.20:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6241 Batch_id=725 Accuracy=71.21:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7404 Batch_id=726 Accuracy=71.21:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7404 Batch_id=726 Accuracy=71.21:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6634 Batch_id=727 Accuracy=71.21:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7202 Batch_id=728 Accuracy=71.22:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7202 Batch_id=728 Accuracy=71.22:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7437 Batch_id=729 Accuracy=71.22:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8005 Batch_id=730 Accuracy=71.23:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8005 Batch_id=730 Accuracy=71.23:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7075 Batch_id=731 Accuracy=71.23:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9639 Batch_id=732 Accuracy=71.22:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9639 Batch_id=732 Accuracy=71.22:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9704 Batch_id=733 Accuracy=71.21:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7812 Batch_id=734 Accuracy=71.22:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7812 Batch_id=734 Accuracy=71.22:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6848 Batch_id=735 Accuracy=71.23:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7544 Batch_id=736 Accuracy=71.23:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7544 Batch_id=736 Accuracy=71.23:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6284 Batch_id=737 Accuracy=71.24:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8295 Batch_id=738 Accuracy=71.25:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8295 Batch_id=738 Accuracy=71.25:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6032 Batch_id=739 Accuracy=71.26:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6665 Batch_id=740 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6665 Batch_id=740 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8818 Batch_id=741 Accuracy=71.26:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9159 Batch_id=742 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9159 Batch_id=742 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6844 Batch_id=743 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8878 Batch_id=744 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8878 Batch_id=744 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7512 Batch_id=745 Accuracy=71.27:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9147 Batch_id=746 Accuracy=71.26:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9147 Batch_id=746 Accuracy=71.26:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8357 Batch_id=747 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8147 Batch_id=748 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7975 Batch_id=749 Accuracy=71.26:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7975 Batch_id=749 Accuracy=71.26:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8732 Batch_id=750 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8414 Batch_id=751 Accuracy=71.24:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8414 Batch_id=751 Accuracy=71.24:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7802 Batch_id=752 Accuracy=71.24:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5493 Batch_id=753 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5493 Batch_id=753 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7908 Batch_id=754 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7262 Batch_id=755 Accuracy=71.25:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7262 Batch_id=755 Accuracy=71.25:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8891 Batch_id=756 Accuracy=71.25:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6753 Batch_id=757 Accuracy=71.25:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6753 Batch_id=757 Accuracy=71.25:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9695 Batch_id=758 Accuracy=71.24:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7549 Batch_id=759 Accuracy=71.24:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7549 Batch_id=759 Accuracy=71.24:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8841 Batch_id=760 Accuracy=71.23:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0580 Batch_id=761 Accuracy=71.23:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0580 Batch_id=761 Accuracy=71.23:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7827 Batch_id=762 Accuracy=71.24:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0021 Batch_id=763 Accuracy=71.23:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0021 Batch_id=763 Accuracy=71.23:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7021 Batch_id=764 Accuracy=71.24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9097 Batch_id=765 Accuracy=71.24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9097 Batch_id=765 Accuracy=71.24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7562 Batch_id=766 Accuracy=71.25:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9588 Batch_id=767 Accuracy=71.23:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9588 Batch_id=767 Accuracy=71.23:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.9889 Batch_id=768 Accuracy=71.22:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8953 Batch_id=769 Accuracy=71.22:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8953 Batch_id=769 Accuracy=71.22:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.5381 Batch_id=770 Accuracy=71.24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7283 Batch_id=771 Accuracy=71.24:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7283 Batch_id=771 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6288 Batch_id=772 Accuracy=71.26:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0132 Batch_id=773 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=1.0132 Batch_id=773 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7777 Batch_id=774 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8298 Batch_id=775 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8298 Batch_id=775 Accuracy=71.24:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6976 Batch_id=776 Accuracy=71.25:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7800 Batch_id=777 Accuracy=71.25:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7800 Batch_id=777 Accuracy=71.25:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.8287 Batch_id=778 Accuracy=71.25:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6641 Batch_id=779 Accuracy=71.25:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.6641 Batch_id=779 Accuracy=71.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7306 Batch_id=780 Accuracy=71.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7770 Batch_id=781 Accuracy=71.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████Epoch 9 Loss=0.7770 Batch_id=781 Accuracy=71.25: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [01:05<00:00, 11.96it/s]
01102025-1111

Test set: Average loss: 0.7785, Accuracy: 7223/10000 (72.23%)

-----------------------------------------------
EPOCH: 10
01102025-1111
Epoch 10 Loss=0.6120 Batch_id=781 Accuracy=72.09: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:43<00:00, 17.95it/s] 
01102025-1112

Test set: Average loss: 0.7743, Accuracy: 7290/10000 (72.90%)

-----------------------------------------------
EPOCH: 11
01102025-1112
Epoch 11 Loss=0.3643 Batch_id=781 Accuracy=72.82: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:42<00:00, 18.40it/s] 
01102025-1112

Test set: Average loss: 0.7487, Accuracy: 7358/10000 (73.58%)

-----------------------------------------------
EPOCH: 12
01102025-1112
Epoch 12 Loss=0.8340 Batch_id=781 Accuracy=73.62: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.38it/s] 
01102025-1113

Test set: Average loss: 0.7467, Accuracy: 7402/10000 (74.02%)

-----------------------------------------------
EPOCH: 13
01102025-1113
Epoch 13 Loss=0.8599 Batch_id=781 Accuracy=73.86: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:42<00:00, 18.39it/s] 
01102025-1114

Test set: Average loss: 0.7576, Accuracy: 7358/10000 (73.58%)

-----------------------------------------------
EPOCH: 14
01102025-1114
Epoch 14 Loss=0.4980 Batch_id=781 Accuracy=74.75: 100%|████████████████████████████████████████████████████████████████████| 782/782 [7:24:35<00:00, 34.11s/it] 
01102025-1838

Test set: Average loss: 0.7289, Accuracy: 7452/10000 (74.52%)

-----------------------------------------------
EPOCH: 15
01102025-1838
Epoch 15 Loss=0.7383 Batch_id=781 Accuracy=75.03: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.40it/s] 
01102025-1839

Test set: Average loss: 0.7334, Accuracy: 7432/10000 (74.32%)

-----------------------------------------------
EPOCH: 16
01102025-1839
Epoch 16 Loss=0.7593 Batch_id=781 Accuracy=75.56: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:32<00:00, 24.33it/s] 
01102025-1840

Test set: Average loss: 0.7271, Accuracy: 7477/10000 (74.77%)

-----------------------------------------------
EPOCH: 17
01102025-1840
Epoch 17 Loss=1.2154 Batch_id=781 Accuracy=75.86: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:34<00:00, 22.60it/s] 
01102025-1840

Test set: Average loss: 0.7028, Accuracy: 7553/10000 (75.53%)

-----------------------------------------------
EPOCH: 18
01102025-1840
Epoch 18 Loss=0.6781 Batch_id=781 Accuracy=76.18: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:34<00:00, 22.88it/s] 
01102025-1841

Test set: Average loss: 0.7304, Accuracy: 7431/10000 (74.31%)

-----------------------------------------------
EPOCH: 19
01102025-1841
Epoch 19 Loss=0.5211 Batch_id=781 Accuracy=76.52: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:35<00:00, 22.26it/s] 
01102025-1841

Test set: Average loss: 0.6921, Accuracy: 7580/10000 (75.80%)

-----------------------------------------------
EPOCH: 20
01102025-1842
Epoch 20 Loss=0.6900 Batch_id=781 Accuracy=77.02: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.37it/s] 
01102025-1842

Test set: Average loss: 0.7179, Accuracy: 7500/10000 (75.00%)

-----------------------------------------------
EPOCH: 21
01102025-1842
Epoch 21 Loss=0.5573 Batch_id=781 Accuracy=77.38: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:34<00:00, 22.62it/s] 
01102025-1843

Test set: Average loss: 0.6905, Accuracy: 7594/10000 (75.94%)

-----------------------------------------------
EPOCH: 22
01102025-1843
Epoch 22 Loss=0.5561 Batch_id=781 Accuracy=77.32: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.53it/s] 
01102025-1843

Test set: Average loss: 0.6716, Accuracy: 7643/10000 (76.43%)

-----------------------------------------------
EPOCH: 23
01102025-1843
Epoch 23 Loss=1.2827 Batch_id=781 Accuracy=77.66: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.58it/s] 
01102025-1844

Test set: Average loss: 0.6805, Accuracy: 7670/10000 (76.70%)

-----------------------------------------------
EPOCH: 24
01102025-1844
Epoch 24 Loss=0.7548 Batch_id=781 Accuracy=77.87: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:38<00:00, 20.30it/s] 
01102025-1845

Test set: Average loss: 0.6893, Accuracy: 7620/10000 (76.20%)

-----------------------------------------------
EPOCH: 25
01102025-1845
Epoch 25 Loss=0.6465 Batch_id=781 Accuracy=78.30: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:39<00:00, 19.94it/s] 
01102025-1845

Test set: Average loss: 0.7246, Accuracy: 7520/10000 (75.20%)

-----------------------------------------------
EPOCH: 26
01102025-1845
Epoch 26 Loss=1.0204 Batch_id=781 Accuracy=78.40: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:38<00:00, 20.42it/s] 
01102025-1846

Test set: Average loss: 0.7038, Accuracy: 7586/10000 (75.86%)

-----------------------------------------------
EPOCH: 27
01102025-1846
Epoch 27 Loss=0.6209 Batch_id=781 Accuracy=78.49: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:38<00:00, 20.26it/s] 
01102025-1847

Test set: Average loss: 0.6734, Accuracy: 7656/10000 (76.56%)

-----------------------------------------------
EPOCH: 28
01102025-1847
Epoch 28 Loss=0.8452 Batch_id=781 Accuracy=79.13: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:38<00:00, 20.06it/s] 
01102025-1847

Test set: Average loss: 0.6751, Accuracy: 7695/10000 (76.95%)

-----------------------------------------------
EPOCH: 29
01102025-1847
Epoch 29 Loss=0.6411 Batch_id=781 Accuracy=79.41: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:37<00:00, 20.65it/s] 
01102025-1848

Test set: Average loss: 0.6484, Accuracy: 7765/10000 (77.65%)

-----------------------------------------------
EPOCH: 30
01102025-1848
Epoch 30 Loss=0.6005 Batch_id=781 Accuracy=79.56: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.50it/s] 
01102025-1849

Test set: Average loss: 0.6670, Accuracy: 7707/10000 (77.07%)

-----------------------------------------------
EPOCH: 31
01102025-1849
Epoch 31 Loss=0.8381 Batch_id=781 Accuracy=79.92: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.58it/s] 
01102025-1849

Test set: Average loss: 0.6583, Accuracy: 7743/10000 (77.43%)

-----------------------------------------------
EPOCH: 32
01102025-1849
Epoch 32 Loss=1.0178 Batch_id=781 Accuracy=80.38: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.39it/s] 
01102025-1850

Test set: Average loss: 0.6696, Accuracy: 7698/10000 (76.98%)

-----------------------------------------------
EPOCH: 33
01102025-1850
Epoch 33 Loss=0.4611 Batch_id=781 Accuracy=80.36: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.20it/s] 
01102025-1851

Test set: Average loss: 0.6560, Accuracy: 7754/10000 (77.54%)

-----------------------------------------------
EPOCH: 34
01102025-1851
Epoch 34 Loss=0.6843 Batch_id=781 Accuracy=80.71: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:37<00:00, 21.05it/s] 
01102025-1851

Test set: Average loss: 0.6550, Accuracy: 7795/10000 (77.95%)

-----------------------------------------------
EPOCH: 35
01102025-1851
Epoch 35 Loss=0.7901 Batch_id=781 Accuracy=81.14: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.19it/s] 
01102025-1852

Test set: Average loss: 0.6482, Accuracy: 7792/10000 (77.92%)

-----------------------------------------------
EPOCH: 36
01102025-1852
Epoch 36 Loss=0.2101 Batch_id=781 Accuracy=81.34: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:37<00:00, 20.99it/s] 
01102025-1853

Test set: Average loss: 0.6611, Accuracy: 7762/10000 (77.62%)

-----------------------------------------------
EPOCH: 37
01102025-1853
Epoch 37 Loss=1.1127 Batch_id=781 Accuracy=81.70: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.23it/s] 
01102025-1853

Test set: Average loss: 0.6299, Accuracy: 7894/10000 (78.94%)

-----------------------------------------------
EPOCH: 38
01102025-1853
Epoch 38 Loss=1.6821 Batch_id=781 Accuracy=82.29: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:45<00:00, 17.09it/s] 
01102025-1854

Test set: Average loss: 0.6334, Accuracy: 7864/10000 (78.64%)

-----------------------------------------------
EPOCH: 39
01102025-1854
Epoch 39 Loss=0.8469 Batch_id=781 Accuracy=82.63: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:47<00:00, 16.56it/s] 
01102025-1855

Test set: Average loss: 0.6521, Accuracy: 7838/10000 (78.38%)

-----------------------------------------------
EPOCH: 40
01102025-1855
Epoch 40 Loss=0.4416 Batch_id=781 Accuracy=82.90: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:54<00:00, 14.41it/s] 
01102025-1856

Test set: Average loss: 0.6396, Accuracy: 7878/10000 (78.78%)

-----------------------------------------------
EPOCH: 41
01102025-1856
Epoch 41 Loss=0.6115 Batch_id=781 Accuracy=83.49: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:51<00:00, 15.08it/s] 
01102025-1857

Test set: Average loss: 0.6218, Accuracy: 7919/10000 (79.19%)

-----------------------------------------------
EPOCH: 42
01102025-1857
Epoch 42 Loss=0.3430 Batch_id=781 Accuracy=84.05: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:50<00:00, 15.60it/s] 
01102025-1858

Test set: Average loss: 0.6201, Accuracy: 7910/10000 (79.10%)

-----------------------------------------------
EPOCH: 43
01102025-1858
Epoch 43 Loss=0.6360 Batch_id=781 Accuracy=84.51: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:48<00:00, 16.11it/s] 
01102025-1858

Test set: Average loss: 0.6134, Accuracy: 7981/10000 (79.81%)

-----------------------------------------------
EPOCH: 44
01102025-1859
Epoch 44 Loss=0.6414 Batch_id=781 Accuracy=84.69: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:47<00:00, 16.37it/s] 
01102025-1859

Test set: Average loss: 0.6271, Accuracy: 7958/10000 (79.58%)

-----------------------------------------------
EPOCH: 45
01102025-1859
Epoch 45 Loss=0.1406 Batch_id=781 Accuracy=85.02: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:46<00:00, 16.93it/s] 
01102025-1900

Test set: Average loss: 0.6167, Accuracy: 7969/10000 (79.69%)

-----------------------------------------------
EPOCH: 46
01102025-1900
Epoch 46 Loss=0.2554 Batch_id=781 Accuracy=85.73: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:48<00:00, 16.15it/s] 
01102025-1901

Test set: Average loss: 0.6174, Accuracy: 8009/10000 (80.09%)

-----------------------------------------------
EPOCH: 47
01102025-1901
Epoch 47 Loss=0.7478 Batch_id=781 Accuracy=85.97: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:48<00:00, 16.29it/s] 
01102025-1902

Test set: Average loss: 0.6157, Accuracy: 8018/10000 (80.18%)

-----------------------------------------------
EPOCH: 48
01102025-1902
Epoch 48 Loss=0.5928 Batch_id=781 Accuracy=86.23: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:46<00:00, 16.98it/s] 
01102025-1903

Test set: Average loss: 0.6122, Accuracy: 8021/10000 (80.21%)

-----------------------------------------------

Test set: Average loss: 0.6076, Accuracy: 8017/10000 (80.17%)

-----------------------------------------------
EPOCH: 50
01102025-1904
Epoch 50 Loss=0.4177 Batch_id=781 Accuracy=86.41: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:49<00:00, 15.86it/s]
01102025-1904

Test set: Average loss: 0.6107, Accuracy: 8026/10000 (80.26%)

-----------------------------------------------
Plot saved as training_results-01102025-1904.png
(.venv) PS C:\Raghu\MyLearnings\ERA_V4\S7-27092025\cifar10_image_classication>

===========================================================================================================================

from __future__ import print_function
import torch
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torchsummary import summary
from datetime import datetime
from ModelCIFAR10 import Net, get_optimizer_and_scheduler
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

class AlbumentationsTransform:
    def __init__(self, transform):
        self.transform = transform

    def __call__(self, img):
        # Albumentations works with numpy, so convert PIL → numpy
        img = np.array(img)
        augmented = self.transform(image=img)
        return augmented["image"]


# Train Phase transformations
# train_transforms = transforms.Compose([
#                                       #  transforms.Resize((28, 28)),
#                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),
#                                       transforms.RandomHorizontalFlip(p=0.5),   # NEW: horizontal flip
#                                       transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),
#                                     #   transforms.Rand
                                                
#                                     #    transforms.RandomRotation((-10.0, 10.0), fill=(1,)),
#                                     #    transforms.RandomAffine(degrees=7, translate=(0.05, 0.05), scale=(0.9, 1.1)),
#                                        transforms.ToTensor(),
#                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.
#                                        # Note the difference between (0.1307) and (0.1307,)
#                                        ])

train_transforms = AlbumentationsTransform(A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),
    A.CoarseDropout(
        max_holes=1, min_holes=1,
        max_height=16, max_width=16,
        min_height=16, min_width=16,
        fill_value=(0.4914, 0.4822, 0.4465),  # CIFAR-10 mean
        p=0.5
    ),
    A.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),
    ToTensorV2(),
]))

# Test Phase transformations
# test_transforms = transforms.Compose([
#                                       #  transforms.Resize((28, 28)),
#                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),
#                                        transforms.ToTensor(),
#                                        transforms.Normalize((0.1307,), (0.3081,))
#                                        ])

test_transforms = AlbumentationsTransform(A.Compose([
    A.Normalize(mean=(0.4914, 0.4822, 0.4465),
                std=(0.2023, 0.1994, 0.2010)),
    ToTensorV2()
]))

"""# Dataset and Creating Train/Test Split"""

# train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)
# test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)

train = datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)
test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)

SEED = 1

# CUDA?
cuda = torch.cuda.is_available()
print("CUDA Available?", cuda)

# For reproducibility
torch.manual_seed(SEED)

if cuda:
    torch.cuda.manual_seed(SEED)

# dataloader arguments - something you'll fetch these from cmdprmt
dataloader_args_train = dict(shuffle=True, batch_size=64, num_workers=0, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)

# train dataloader
train_loader = torch.utils.data.DataLoader(train, **dataloader_args_train)

dataloader_args_test = dict(shuffle=False, batch_size=1000, num_workers=0, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)
# test dataloader
test_loader = torch.utils.data.DataLoader(test, **dataloader_args_test)


use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print(device)
model = Net().to(device)
summary(model, input_size=(3, 32, 32))


from tqdm import tqdm

train_losses = []
test_losses = []
train_acc = []
test_acc = []


def train(model, device, train_loader, optimizer, scheduler, epoch):
  model.train()
  pbar = tqdm(train_loader)
  correct = 0
  processed = 0
  running_loss = 0.0
  for batch_idx, (data, target) in enumerate(pbar):
    # get samples
    data, target = data.to(device), target.to(device)

    # Init
    optimizer.zero_grad()
    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.
    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.

    # Predict
    y_pred = model(data)

    # Calculate loss
    loss = F.nll_loss(y_pred, target)
    # train_losses.append(loss.item())

    # Backpropagation
    loss.backward()
    optimizer.step()

    scheduler.step()
    running_loss += loss.item()

    # Update pbar-tqdm

    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
    correct += pred.eq(target.view_as(pred)).sum().item()
    processed += len(data)

    pbar.set_description(
        desc=f'Epoch {epoch} Loss={loss.item():.4f} '
             f'Batch_id={batch_idx} '
             f'Accuracy={100*correct/processed:0.2f}'
    )

    # train_losses.append(loss / len(train_loader))
    train_losses.append(loss.item()) 
    train_acc.append(100. * correct / processed)

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)

    test_losses.append(test_loss)
    test_acc.append(accuracy)

    print(f"\nTest set: Average loss: {test_loss:.4f}, "
          f"Accuracy: {correct}/{len(test_loader.dataset)} "
          f"({accuracy:.2f}%)\n")

    return test_loss   



model =  Net().to(device)
optimizer, scheduler = get_optimizer_and_scheduler(model,len(train_loader), EPOCHS=50)

if __name__ == "__main__":
    EPOCHS = 50
    for epoch in range(EPOCHS):
        print("EPOCH:", epoch+1)
        print(datetime.now().strftime("%d%m%Y-%H%M"))
        train(model, device, train_loader, optimizer, scheduler, epoch+1)
        print(datetime.now().strftime("%d%m%Y-%H%M"))
        val_loss=test(model, device, test_loader)
        
        print("-----------------------------------------------")
    import matplotlib.pyplot as plt
    fig, axs = plt.subplots(2,2,figsize=(15,10))

    # axs[0, 0].plot(train_losses)
    axs[0, 0].plot([loss for loss in train_losses], label='Training Loss')
    axs[0, 0].set_title("Training Loss")
    axs[0, 0].legend()

    axs[1, 0].plot([acc for acc in train_acc], label="Train Accuracy")
    axs[1, 0].set_title("Training Accuracy")
    axs[1, 0].legend()
    
    axs[0, 1].plot([loss for loss in test_losses], label="Test Loss", color="orange")
    axs[0, 1].set_title("Test Loss")
    axs[0, 1].legend()

    axs[1, 1].plot([acc for acc in test_acc], label="Test Accuracy", color="green")
    axs[1, 1].set_title("Test Accuracy")
    axs[1, 1].legend()
        
    # Format timestamp as DDMMYYYY-HHMM
    
    timestamp = datetime.now().strftime("%d%m%Y-%H%M")

    # Save with timestamp in filename
    filename = f"training_results-{timestamp}.png"
    plt.tight_layout()
    plt.savefig(filename, dpi=300)

    print(f"Plot saved as {filename}")
    # plt.show()

===========================================================================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)
        self.bn1   = nn.BatchNorm2d(32)

        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
        self.bn2   = nn.BatchNorm2d(64)

        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1)
        self.bn3   = nn.BatchNorm2d(32)

        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
        self.bn4   = nn.BatchNorm2d(64)

        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
        self.bn5   = nn.BatchNorm2d(128)

        self.conv1x1 = nn.Conv2d(128, 10, kernel_size=1)

        self.gap = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))   # 3 -> 32
        x = F.relu(self.bn2(self.conv2(x)))   # 32 -> 64
        x = F.relu(self.bn3(self.conv3(x)))   # 64 -> 32
        x = F.relu(self.bn4(self.conv4(x)))   # 32 -> 64
        x = F.relu(self.bn5(self.conv5(x)))   # 64 -> 128

        x = self.conv1x1(x)                   # 128 -> 10
        x = self.gap(x)                       # GAP
        x = x.view(-1, 10)
        return F.log_softmax(x, dim=1)

def get_optimizer_and_scheduler(model, train_loader_len, EPOCHS,lr=0.01, momentum=0.9,):
    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, steps_per_epoch=train_loader_len, epochs=EPOCHS, anneal_strategy='cos',pct_start=0.2,div_factor=10.0,final_div_factor=100.0 )
    return optimizer, scheduler

===========================================================================================================================