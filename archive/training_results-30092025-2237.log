(.venv) PS C:\Raghu\MyLearnings\ERA_V4\S7-27092025\cifar10_image_classication> python .\ExperimentCIFAR10.py    
C:\Raghu\MyLearnings\ERA_V4\S7-27092025\cifar10_image_classication\.venv\Lib\site-packages\albumentations\core\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.
  original_init(self, **validated_kwargs)
C:\Raghu\MyLearnings\ERA_V4\S7-27092025\cifar10_image_classication\ExperimentCIFAR10.py:42: UserWarning: Argument(s) 'max_holes, min_holes, max_height, max_width, min_height, min_width, fill_value' are not valid for transform CoarseDropout
  A.CoarseDropout(
Files already downloaded and verified
Files already downloaded and verified
CUDA Available? True
cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 32, 32]             224
       BatchNorm2d-2            [-1, 8, 32, 32]              16
            Conv2d-3           [-1, 16, 32, 32]           1,168
       BatchNorm2d-4           [-1, 16, 32, 32]              32
         MaxPool2d-5           [-1, 16, 16, 16]               0
            Conv2d-6           [-1, 16, 16, 16]           2,320
       BatchNorm2d-7           [-1, 16, 16, 16]              32
         MaxPool2d-8             [-1, 16, 8, 8]               0
            Conv2d-9             [-1, 28, 8, 8]           4,060
      BatchNorm2d-10             [-1, 28, 8, 8]              56
           Conv2d-11             [-1, 10, 8, 8]             290
AdaptiveAvgPool2d-12             [-1, 10, 1, 1]               0
================================================================
Total params: 8,198
Trainable params: 8,198
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.51
Params size (MB): 0.03
Estimated Total Size (MB): 0.55
----------------------------------------------------------------
EPOCH: 1
30092025-2226
Epoch 1 Loss=1.8075 Batch_id=781 Accuracy=31.20: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:34<00:00, 22.89it/s]
30092025-2227

Test set: Average loss: 1.5507, Accuracy: 4381/10000 (43.81%)

-----------------------------------------------
EPOCH: 2
30092025-2227
Epoch 2 Loss=0.9783 Batch_id=781 Accuracy=45.70: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:35<00:00, 21.92it/s] 
30092025-2228

Test set: Average loss: 1.8152, Accuracy: 3682/10000 (36.82%)

-----------------------------------------------
EPOCH: 3
30092025-2228
Epoch 3 Loss=1.2191 Batch_id=781 Accuracy=52.05: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:36<00:00, 21.22it/s] 
30092025-2228

Test set: Average loss: 1.4011, Accuracy: 5003/10000 (50.03%)

-----------------------------------------------
EPOCH: 4
30092025-2228
Epoch 4 Loss=1.3042 Batch_id=781 Accuracy=55.35: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:38<00:00, 20.15it/s] 
30092025-2229

Test set: Average loss: 1.1895, Accuracy: 5730/10000 (57.30%)

-----------------------------------------------
EPOCH: 5
30092025-2229
Epoch 5 Loss=1.1921 Batch_id=781 Accuracy=58.12: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:39<00:00, 19.79it/s] 
30092025-2230

Test set: Average loss: 1.1171, Accuracy: 5956/10000 (59.56%)

-----------------------------------------------
EPOCH: 6
30092025-2230
Epoch 6 Loss=0.8440 Batch_id=781 Accuracy=59.42: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:40<00:00, 19.20it/s] 
30092025-2230

Test set: Average loss: 1.1541, Accuracy: 5856/10000 (58.56%)

-----------------------------------------------
EPOCH: 7
30092025-2230
Epoch 7 Loss=0.9264 Batch_id=781 Accuracy=60.73: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:42<00:00, 18.22it/s] 
30092025-2231

Test set: Average loss: 1.0958, Accuracy: 6084/10000 (60.84%)

-----------------------------------------------
EPOCH: 8
30092025-2231
Epoch 8 Loss=1.0797 Batch_id=781 Accuracy=62.30: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:41<00:00, 18.74it/s] 
30092025-2232

Test set: Average loss: 1.0691, Accuracy: 6199/10000 (61.99%)

-----------------------------------------------
EPOCH: 9
30092025-2232
Epoch 9 Loss=1.3133 Batch_id=781 Accuracy=62.74: 100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:42<00:00, 18.30it/s] 
30092025-2233

Test set: Average loss: 0.9820, Accuracy: 6455/10000 (64.55%)

-----------------------------------------------
EPOCH: 10
30092025-2233
Epoch 10 Loss=0.9013 Batch_id=781 Accuracy=63.36: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.64it/s] 
30092025-2233

Test set: Average loss: 1.0507, Accuracy: 6315/10000 (63.15%)

-----------------------------------------------
EPOCH: 11
30092025-2233
Epoch 11 Loss=1.0898 Batch_id=781 Accuracy=64.20: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.64it/s] 
30092025-2234

Test set: Average loss: 0.9943, Accuracy: 6431/10000 (64.31%)

-----------------------------------------------
EPOCH: 12
30092025-2234
Epoch 12 Loss=0.8464 Batch_id=781 Accuracy=65.15: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:45<00:00, 17.02it/s] 
30092025-2235

Test set: Average loss: 0.9152, Accuracy: 6710/10000 (67.10%)

-----------------------------------------------
EPOCH: 13
30092025-2235
Epoch 13 Loss=1.0641 Batch_id=781 Accuracy=65.87: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.72it/s] 
30092025-2236

Test set: Average loss: 0.8928, Accuracy: 6779/10000 (67.79%)

-----------------------------------------------
EPOCH: 14
30092025-2236
Epoch 14 Loss=0.7172 Batch_id=781 Accuracy=66.35: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.66it/s] 
30092025-2237


-----------------------------------------------
EPOCH: 15
30092025-2237
Epoch 15 Loss=0.8512 Batch_id=781 Accuracy=66.62: 100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:44<00:00, 17.48it/s]
30092025-2237

Test set: Average loss: 0.8775, Accuracy: 6826/10000 (68.26%)

-----------------------------------------------
Plot saved as training_results-30092025-2237.png
(.venv) PS C:\Raghu\MyLearnings\ERA_V4\S7-27092025\cifar10_image_classication>

===========================================================================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR

padding_value=1
stride_value=1
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=stride_value, padding=padding_value)
        self.bn1   = nn.BatchNorm2d(num_features=8)

        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16, kernel_size=3,stride=stride_value, padding=padding_value)
        self.bn2   = nn.BatchNorm2d(num_features=16)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  

        self.conv3 = nn.Conv2d(in_channels=16,out_channels=16, kernel_size=3,stride=stride_value, padding=padding_value)
        self.bn3 = nn.BatchNorm2d(num_features=16)

        self.conv4 = nn.Conv2d(in_channels=16,out_channels=28, kernel_size=3,stride=stride_value, padding=padding_value)
        self.bn4 = nn.BatchNorm2d(num_features=28)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv1_1 = nn.Conv2d(in_channels=28, out_channels=10, kernel_size=1) # 1x1 conv to get 10 channels for 10 classes

        self.gap = nn.AdaptiveAvgPool2d((1, 1))  


    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x))) # 1x28x28 -> 8x28x28
        
        x = F.relu(self.bn2(self.conv2(x))) # 8x28x28 -> 16x28x28        
        x = self.pool1(x)                   # 16x28x28 -> 16x14x14

        x = F.relu(self.bn3(self.conv3(x))) # 16x14x14 -> 16x14x14
        x = self.pool2(x)                   # 16x14x14 -> 16x7x7

        x = F.relu(self.bn4(self.conv4(x))) # 16x7x7 -> 32x7x7

        #1 x1 convolution to get 10 channels for 10 classes
        x = self.conv1_1(x)                # 32x7x7 -> 10x7x7
        
        # Global Average Pooling
        x = self.gap(x)                     # 10x7x7 -> 10x1x1

        # Flatten
        
        x = x.view(-1, 10)
        return F.log_softmax(x, dim=1)



def get_optimizer_and_scheduler(model, train_loader_len, EPOCHS,lr=0.01, momentum=0.9,):
    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.015, steps_per_epoch=train_loader_len, epochs=EPOCHS, anneal_strategy='cos',pct_start=0.2,div_factor=10.0,final_div_factor=100.0 )
    return optimizer, scheduler
===========================================================================================================================

from __future__ import print_function
import torch
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torchsummary import summary
from datetime import datetime
from ModelCIFAR10 import Net, get_optimizer_and_scheduler
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

class AlbumentationsTransform:
    def __init__(self, transform):
        self.transform = transform

    def __call__(self, img):
        # Albumentations works with numpy, so convert PIL → numpy
        img = np.array(img)
        augmented = self.transform(image=img)
        return augmented["image"]


# Train Phase transformations
 train_transforms = transforms.Compose([
                                       #  transforms.Resize((28, 28)),
                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),
                                       transforms.RandomHorizontalFlip(p=0.5),   # NEW: horizontal flip
                                       transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                                                                    
                                     #    transforms.RandomRotation((-10.0, 10.0), fill=(1,)),
                                     #    transforms.RandomAffine(degrees=7, translate=(0.05, 0.05), scale=(0.9, 1.1)),
                                        transforms.ToTensor(),
                                        transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.
                                        # Note the difference between (0.1307) and (0.1307,)
                                        ])


# Test Phase transformations
 test_transforms = transforms.Compose([
                                       #  transforms.Resize((28, 28)),
                                       #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),
                                        transforms.ToTensor(),
                                        transforms.Normalize((0.1307,), (0.3081,))
                                        ])


"""# Dataset and Creating Train/Test Split"""

# train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)
# test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)

train = datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)
test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)

SEED = 1

# CUDA?
cuda = torch.cuda.is_available()
print("CUDA Available?", cuda)

# For reproducibility
torch.manual_seed(SEED)

if cuda:
    torch.cuda.manual_seed(SEED)

# dataloader arguments - something you'll fetch these from cmdprmt
dataloader_args_train = dict(shuffle=True, batch_size=64, num_workers=0, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)

# train dataloader
train_loader = torch.utils.data.DataLoader(train, **dataloader_args_train)

dataloader_args_test = dict(shuffle=False, batch_size=1000, num_workers=0, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)
# test dataloader
test_loader = torch.utils.data.DataLoader(test, **dataloader_args_test)


use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print(device)
model = Net().to(device)
summary(model, input_size=(3, 32, 32))


from tqdm import tqdm

train_losses = []
test_losses = []
train_acc = []
test_acc = []


def train(model, device, train_loader, optimizer, scheduler, epoch):
  model.train()
  pbar = tqdm(train_loader)
  correct = 0
  processed = 0
  running_loss = 0.0
  for batch_idx, (data, target) in enumerate(pbar):
    # get samples
    data, target = data.to(device), target.to(device)

    # Init
    optimizer.zero_grad()
    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.
    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.

    # Predict
    y_pred = model(data)

    # Calculate loss
    loss = F.nll_loss(y_pred, target)
    # train_losses.append(loss.item())

    # Backpropagation
    loss.backward()
    optimizer.step()

    scheduler.step()
    running_loss += loss.item()

    # Update pbar-tqdm

    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
    correct += pred.eq(target.view_as(pred)).sum().item()
    processed += len(data)

    pbar.set_description(
        desc=f'Epoch {epoch} Loss={loss.item():.4f} '
             f'Batch_id={batch_idx} '
             f'Accuracy={100*correct/processed:0.2f}'
    )

    # train_losses.append(loss / len(train_loader))
    train_losses.append(loss.item()) 
    train_acc.append(100. * correct / processed)

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)

    test_losses.append(test_loss)
    test_acc.append(accuracy)

    print(f"\nTest set: Average loss: {test_loss:.4f}, "
          f"Accuracy: {correct}/{len(test_loader.dataset)} "
          f"({accuracy:.2f}%)\n")

    return test_loss   



model =  Net().to(device)
optimizer, scheduler = get_optimizer_and_scheduler(model,len(train_loader), EPOCHS=15)

if __name__ == "__main__":
    EPOCHS = 15
    for epoch in range(EPOCHS):
        print("EPOCH:", epoch+1)
        print(datetime.now().strftime("%d%m%Y-%H%M"))
        train(model, device, train_loader, optimizer, scheduler, epoch+1)
        print(datetime.now().strftime("%d%m%Y-%H%M"))
        val_loss=test(model, device, test_loader)
        
        print("-----------------------------------------------")
    import matplotlib.pyplot as plt
    fig, axs = plt.subplots(2,2,figsize=(15,10))

    # axs[0, 0].plot(train_losses)
    axs[0, 0].plot([loss for loss in train_losses], label='Training Loss')
    axs[0, 0].set_title("Training Loss")
    axs[0, 0].legend()

    axs[1, 0].plot([acc for acc in train_acc], label="Train Accuracy")
    axs[1, 0].set_title("Training Accuracy")
    axs[1, 0].legend()
    
    axs[0, 1].plot([loss for loss in test_losses], label="Test Loss", color="orange")
    axs[0, 1].set_title("Test Loss")
    axs[0, 1].legend()

    axs[1, 1].plot([acc for acc in test_acc], label="Test Accuracy", color="green")
    axs[1, 1].set_title("Test Accuracy")
    axs[1, 1].legend()
        
    # Format timestamp as DDMMYYYY-HHMM
    
    timestamp = datetime.now().strftime("%d%m%Y-%H%M")

    # Save with timestamp in filename
    filename = f"training_results-{timestamp}.png"
    plt.tight_layout()
    plt.savefig(filename, dpi=300)

    print(f"Plot saved as {filename}")
    # plt.show()
===========================================================================================================================